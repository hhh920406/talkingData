{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_label(df, group_cols):\n",
    "    col_name = \"_\".join(group_cols)\n",
    "    group_idx = df.drop_duplicates(group_cols)[group_cols].reset_index()\n",
    "    group_idx.rename(columns={'index':col_name}, inplace=True)\n",
    "    df = df.merge( group_idx, on=group_cols, how='left' )\n",
    "    del group_idx\n",
    "    gc.collect()\n",
    "    return df\n",
    "    \n",
    "def count_agg(df, group_cols):\n",
    "    col_name = \"_\".join(group_cols)+'_count'\n",
    "    count = df.groupby(group_cols).size().reset_index(name=col_name)\n",
    "    df = df.merge(count, on=group_cols, how='left')\n",
    "    del count\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def count_cum(df, group_cols):\n",
    "    col_name = \"_\".join(group_cols)+'_countAccum'\n",
    "    df[col_name] = df.groupby(group_cols).cumcount()\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def count_uniq(df, group_cols, uniq_col):\n",
    "    col_name = \"_\".join(group_cols)+'_uniq_'+uniq_col+'_countUniq'\n",
    "    tmp = df.groupby(group_cols)[uniq_col].nunique().reset_index(name=col_name)\n",
    "    df = df.merge(tmp, on=group_cols, how='left')\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def next_click(df, group_cols):\n",
    "    df[\"_\".join(group_cols)+'_nextClick'] = (df.groupby(group_cols).click_time.shift(-1) - df.click_time).astype(np.float32)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def frequence(df, group_cols):\n",
    "    col_name = \"_\".join(group_cols)+'_nextClick'\n",
    "    clickFreq = df.groupby(group_cols)[col_name].mean().dropna().reset_index(name=(\"_\".join(group_cols)+'_clickFreq'))\n",
    "    df = df.merge(clickFreq, on=group_cols, how='left')\n",
    "    del clickFreq\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# accumulated count, need sorted df by click time\n",
    "def count_cum(df, group_cols):\n",
    "    col_name = \"_\".join(group_cols)+'_countAccum'\n",
    "    df[col_name] = df.groupby(group_cols).cumcount()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    print('generating time features...')\n",
    "    df['day'] = df['click_time'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['click_time'].dt.hour.astype('uint8')\n",
    "    df['in_test_hh'] = (3 - 2 * df['hour'].isin([4, 5, 9, 10, 13, 14]) # most frequent\n",
    "                          - 1 * df['hour'].isin([6, 11, 15])).astype('uint8') # least frequent\n",
    "    print('done')\n",
    "    gc.collect()\n",
    "    \n",
    "    group_combinations = [\n",
    "        ['app', 'device'],\n",
    "        ['app', 'channel']\n",
    "    ]\n",
    "    \n",
    "    count_combinations = [\n",
    "        ['app'],\n",
    "        ['ip'], # 3.03\n",
    "        ['channel'],\n",
    "        ['os'],\n",
    "        ['ip', 'device'], # 9.88\n",
    "        ['day', 'hour', 'app'], # 4.08\n",
    "        ['app', 'channel'], # 2.8\n",
    "        ['ip', 'day', 'in_test_hh'], # 1.74\n",
    "        ['ip', 'day', 'hour'], # 0.52\n",
    "        ['os', 'device'], # 0.44\n",
    "        ['ip', 'os', 'day', 'hour'], # 0.41\n",
    "        ['ip', 'device', 'day', 'hour'], # 0.31\n",
    "        ['ip', 'app', 'os'] # 0.21\n",
    "    ]\n",
    "    \n",
    "    countUniq_combinations = [\n",
    "        #[['app'],'ip'],\n",
    "        #[['app', 'device', 'os', 'channel'], 'ip'],\n",
    "        [['ip'], 'channel'], # 0.9\n",
    "        [['ip'], 'app'], # 1.3\n",
    "        [['ip'], 'os'] # 0.45\n",
    "    ]\n",
    "    \n",
    "    nextClick_combinations = [\n",
    "        ['ip', 'os'],\n",
    "        ['ip', 'device', 'os'],\n",
    "        ['ip', 'app', 'device', 'os'],\n",
    "        ['ip', 'app', 'device', 'os', 'channel']\n",
    "    ]\n",
    "    \n",
    "    freq_combinations = [\n",
    "        #['ip', 'app', 'device', 'os']\n",
    "    ]\n",
    "    \n",
    "    accum_combinations = [\n",
    "        #['app'],\n",
    "        ['ip'] # 3.03\n",
    "        #['day', 'hour', 'app']\n",
    "    ]\n",
    "    \n",
    "    # group labels\n",
    "    for i, cols in enumerate(group_combinations):\n",
    "        print(i, cols)\n",
    "        df = group_label(df, cols)\n",
    "    \n",
    "    # count features\n",
    "    for i, cols in enumerate(count_combinations):\n",
    "        print(i, cols)\n",
    "        df = count_agg(df, cols)\n",
    "\n",
    "    # count unique features\n",
    "    for i, cols in enumerate(countUniq_combinations):\n",
    "        print(i, cols)\n",
    "        df = count_uniq(df, cols[0], cols[1])\n",
    "    \n",
    "    # next click features\n",
    "    df['click_time'] = (df['click_time'].astype(np.int64) // 10 ** 9).astype(np.int32)\n",
    "    for i, cols in enumerate(nextClick_combinations):\n",
    "        print(i, cols)\n",
    "        df = next_click(df, cols)\n",
    "    \n",
    "    # click frequence\n",
    "    for i, cols in enumerate(freq_combinations):\n",
    "        print(i, cols)\n",
    "        df = frequence(df, cols)\n",
    "    \n",
    "    # accum count\n",
    "    for i, cols in enumerate(accum_combinations):\n",
    "        print(i, cols)\n",
    "        df = count_cum(df, cols)\n",
    "    \n",
    "    df.drop(['ip', 'click_time', 'day', 'in_test_hh'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'ip' :'uint32',\n",
    "    'app' :'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os' :'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8',\n",
    "    'click_id': 'uint32',\n",
    "}\n",
    "\n",
    "# train: (184903890, 7)\n",
    "# test: (18790469, 7)\n",
    "train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "train_df = pd.read_csv('data/train.csv', dtype=dtype, usecols=train_cols, parse_dates=['click_time'])\n",
    "\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "# using test_supplement \n",
    "test_df = pd.read_csv('data/test_supplement.csv', dtype=dtype, usecols=test_cols, parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine train and test data\n",
    "common_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "all_df = pd.concat([train_df[common_cols], test_df[common_cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "all_df = generate_features(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test features from concated data\n",
    "train_features = all_df.iloc[:train_df.shape[0]]\n",
    "test_features = all_df.iloc[train_df.shape[0]:]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = 'auc'\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': metrics,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 7,\n",
    "    'max_depth': 4,\n",
    "    'min_child_samples': 100,\n",
    "    'max_bin': 100,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 0,\n",
    "    'min_split_gain': 0,\n",
    "    'nthread': 24,\n",
    "    'verbose': 1,\n",
    "    'scale_pos_weight': 200\n",
    "}\n",
    "\n",
    "target = 'is_attributed'\n",
    "features = [col for col in train_features.columns if col not in ['level_0', 'index', 'is_attributed']]\n",
    "category = ['app', 'device', 'os', 'channel', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train valid split\n",
    "labels = train_df.is_attributed.values\n",
    "train_features, valid_features = train_test_split(train_features, test_size=5000000, shuffle=False)\n",
    "train_labels, valid_labels = train_test_split(labels, test_size=5000000, shuffle=False)\n",
    "print('Train size:', len(train_features))\n",
    "print('Valid size:', len(valid_features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data into dataset. Warning: Memory Peak\n",
    "xgtrain = lgb.Dataset(train_features[features].values, \n",
    "                      label=train_labels,\n",
    "                      feature_name=features,\n",
    "                      categorical_feature=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgvalid = lgb.Dataset(valid_features[features].values, \n",
    "                      label=valid_labels,\n",
    "                      feature_name=features,\n",
    "                      categorical_feature=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "evals_results = {}\n",
    "model = lgb.train(lgb_params,\n",
    "                  xgtrain,\n",
    "                  valid_sets=[xgvalid],\n",
    "                  valid_names=['valid'],\n",
    "                  evals_result=evals_results,\n",
    "                  num_boost_round=2000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=1,\n",
    "                  feval=None)\n",
    "n_estimators = model.best_iteration\n",
    "\n",
    "print('\\nModel Info:')\n",
    "print('n_estimators:', n_estimators)\n",
    "print(metrics + ':', evals_results['valid'][metrics][n_estimators - 1])\n",
    "\n",
    "gain = model.feature_importance('gain')\n",
    "ft = pd.DataFrame({'feature': model.feature_name(), 'split': model.feature_importance('split'),\n",
    "                   'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "ft.to_csv('feature_importance_ref.csv', index=False)\n",
    "print(ft)\n",
    "\n",
    "model_name = 'model-%s' % strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model.save_model(model_name)\n",
    "print('model saved as %s' % model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep train lgb model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting...')\n",
    "test_df['is_attributed'] = model.predict(test_features[features], num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('loading test')\n",
    "test = pd.read_csv('data/test.csv', dtype=dtype, usecols=test_cols, parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('merging test_supplement to test')\n",
    "join_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "all_cols = join_cols + ['is_attributed']\n",
    "\n",
    "test = test.merge(test_df[all_cols], how='left', on=join_cols)\n",
    "test = test.drop_duplicates(subset=['click_id'])\n",
    "\n",
    "print(\"Writing the submission data into a csv file...\")\n",
    "test[['click_id', 'is_attributed']].to_csv('submit_lgb_885.gz', index=False, float_format='%.9f', compression='gzip')\n",
    "print(\"All done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.1,\n",
    "              'tree_method': \"hist\",\n",
    "              'grow_policy': \"lossguide\",\n",
    "              # 'max_leaves': 1400,  \n",
    "              'max_depth': 4, \n",
    "              'subsample': 0.7, \n",
    "              'colsample_bytree': 0.7, \n",
    "              'colsample_bylevel':0.7,\n",
    "              'min_child_weight':0,\n",
    "              'alpha':0,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc',\n",
    "              'nthread':24,\n",
    "              'random_state': 42,\n",
    "              'scale_pos_weight':200,\n",
    "              'silent': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train valid split\n",
    "labels = train_df.is_attributed.values\n",
    "train_features, valid_features = train_test_split(train_features, train_size=.95, shuffle=False)\n",
    "train_labels, valid_labels = train_test_split(labels, train_size=.95, shuffle=False)\n",
    "print('Train size:', len(train_features))\n",
    "print('Valid size:', len(valid_features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_features, train_labels)\n",
    "dvalid = xgb.DMatrix(valid_features, valid_labels)\n",
    "watchlist = [(dvalid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(xgb_params, \n",
    "                      dtrain, \n",
    "                      num_boost_round=2000, \n",
    "                      evals=watchlist, \n",
    "                      maximize=True, \n",
    "                      early_stopping_rounds = 50, \n",
    "                      verbose_eval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(xgb_model.get_fscore().items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xgb-model-%s' % strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "xgb_model.save_model(model_name)\n",
    "print('model saved as %s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting...')\n",
    "test_df['is_attributed'] = xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('loading test')\n",
    "test = pd.read_csv('data/test.csv', dtype=dtype, usecols=test_cols, parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('merging test_supplement to test')\n",
    "join_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "all_cols = join_cols + ['is_attributed']\n",
    "\n",
    "test = test.merge(test_df[all_cols], how='left', on=join_cols)\n",
    "test = test.drop_duplicates(subset=['click_id'])\n",
    "\n",
    "print(\"Writing the submission data into a csv file...\")\n",
    "test[['click_id', 'is_attributed']].to_csv('submit_xgb_895.gz', index=False, float_format='%.9f', compression='gzip')\n",
    "print(\"All done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
